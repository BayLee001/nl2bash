%!TEX root=writeup.tex
\section{Implementation}

\subsection{Obtaining Command Grammar}
As presented in \autoref{fig:arch}, one input for the system to obtain a candidate command is the command syntax and it is generated from man pages. The grammar of a command is defined in synopsis section of the man-page, and parsing the section of a command can obtain the desired command signature of a command. 

However, this process is done semi-automated instead of fully automated, due to the fact that man pages of different commands are not following the same structure and the grammar of some commands are conflicted to each other. For example, the man page of ``find'' does not provide all information of a command in the synopsis section, instead, it uses a none-terminal in ``expression'' in its grammar in the synopsis section, and reasoned in text that readers should refer to the ``primary'' section to learn how ``expression'' is expanded into terminals. As a result, we first parse the synopsis and then manually clean them before feeding to the system.

\subsection{Training Data}

The trade-off of choosing training data is between the data size and the data quality. On the one hand, the online forum StackOverflow has a large volume of question/command pairs (~30,000 raw data, and ~2,000 after simple cleaning), but these data are noisy: some of the questions are not well described and the expert obtained the questioner's intent though conversations, and as a result, the text/command pairs obtained are of low quality (not well corresponded). On the other hand, a relatively small amount of clean data can be obtained by manually collecting from tutorials or forums. These data are of relatively high quality, but the data size is pretty small (~200).

Between these two choices, we choose the former, as we consider larger dataset can cover more command grammars and contain enough interesting patterns for the model to learn from. Specifically, we collected ~30,000 text/command pairs from StackOverflow, and perform an initial data cleaning before training, the details of data cleaning is presented below.

\todo{Adding them into future work section.} Linux man pages are well formatted and contains rich natural language text that explains the usage of each command template and its possible arguments. We propose to use the command-explanation pairs extracted from man pages as additional signals to guide the search for high score rankings, thereby remedies the lack of well-formed training pairs.

\subsection{Data Cleaning and Preprocessing}