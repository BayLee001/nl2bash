%!TEX root=writeup.tex
\section{Discussion and Future Work}

Automatically generating bash commands based on natural language descriptions is a non-trivial AI task that fuses both natural language processing and program synthesis techniques. Based on the current result, we identify the following problems in our approach that lead to unsatisfactory result, and plans to fix these problems are listed along with the problems below.

\subsection{Dataset}

Using only noisy StackOverflow data as training data can lead to less satisfactory trained model due to the fact that the data size is still not big enough and the data quality are relatively low. Concretely, small data size results in the coverage issue: some command usage patterns are not covered in the dataset and the trained model cannot identify text describing these patterns. The low quality issue (containing mismatches between commands and texts) make the model learned some wrong translation patterns and thus damages translation quality. 

Based on this observation, we are planning to 1) enrich the current training data set with more (possibly noisy) examples to enhance the coverage of bash command usage and 2) collect a small but high quality data used as a separate training data source. The data sources we plan to obtain are listed below.
\begin{itemize}\itemsep-1pt
\item \textit{SuperUser forum.} SuperUser contains similar data as StackOverflow, but more specialized for system users, and thus can be used as complementary data to enrich the training data set.
\item \textit{Examples from Linux man page.} Besides defining the grammar of a command and explaining its usage, a Linux man page for a command also contain a section providing examples to demonstrate the usage of the command. Though small in the number, these examples are high quality training data that can be utilized, and they can be obtained by parsing the man pages.
\item \textit{Tutorial Examples.} Online bash command tutorials\footnote{http://ryanstutorials.net/linuxtutorial/filemanipulation.php}\footnote{http://www.ee.surrey.ac.uk/Teaching/Unix/} often contain a small set of clean explanation/command pairs. Due to the difficulty of automatically parsing different websites, we are planning to manually extract them from websites to create a data set containing  $\sim$300 examples.
\item \textit{Collecting data form colleagues.} One possibility of obtaining high quality data is to collect from colleagues who are familiar with bash commands. A ``command line programming challenge'' can be hosted to attract experts to write bash commands for our pre-selected natural language descriptions to obtain these data. We consider this method a backup that we are not planning to use in short-term future, but if other data sources are still not satisfactory, we shall come to this method. 
\end{itemize}

\subsection{Training Features}
\label{future:features}

In our current state of model training, only text-term co-occurrence feature is used and this limits the potential of the model. The following other features are planned to be added in training. And the way to combine different features in the system is to learn a linear scoring function to sum up the scores from different features.
\begin{itemize}\itemsep-1pt
\item \textit{Naturalness.} Naturalness is of a command is measured by the co-occurrences of terms in a command. The inspiration of using this feature is that some generated commands contains unnatural term combinations. For example, a command \code{grep -i -l -o -w} is generated by the current approach with the input text ``Find all files from that contain word eth0.'', however, in this case, since the combination of \code{-l} and \code{-i} is not common in writing commands with \code{grep}. And the target of evaluating the naturalness is to help prevent a generated command with abnormal patterns.
\item \textit{TF-IDF scoring with man page.} Man pages provide explanations of all the options (flags, arguments) that can appear in a command. Thus, given texts describing what the desired command should looks like, comparing the similarity of user's text with the man page explanation can help locate what are the options may need to be used in the command. One natural way to compute the similarity is to first compute the TF-IDF score of the words in man pages, and them look up whether words in the user's text are top ranking words in a (option, explanation) pair, if it is, ``option'' is likely to be used in the generated command. This feature can help compensate some options that rarely appear in the training data, as (option, explanation) pairs are complete for all options.
\end{itemize}

\subsection{Modeling Values}
\label{future:semantics}

In the current translation system, arguments in an generated command are not values extracted from the text but a dummy value with its type and users need to manually fill these dummy values. Based on our observation on text/command pairs in the data set, values in a command can come from the following sources: 1) a numeric or ordinal value in the text, e.g. a number `100' or a string `foo.txt' , 2) an implicit value requiring domain knowledge, e.g. current directory ``.'', root folder ``$\sim$''.

To enable a program to instantiate arguments during translation, the following processes are supposed to be added. 
\begin{itemize}\itemsep-1pt
\item Training a model to extract noun phrases in a command that represent values, which can be trained from the same data that is used to train the translation process.
\item Encoding domain knowledge about values as a dictionary that maps concepts into concrete values. E.g. file name, file date, file size, folder, subfolder, hidden file, executable file etc.
\end{itemize}

In some cases, the description text provided by the user may be ambiguous or unclear so that values cannot be directly learned from it. For example, an user asking the question ``how to copy the log file to another directory'' does not contains the information about what the concrete file or folder are. Thus, an interactive phase can be added to ask user to disambiguate the value in the question. For example, in this case, when the probability of ``the log file'' to be the actual value is low, we shall ask the user a question ``What does `the log file' in the question stands for?'', and the user may answer ``xxx.log'' so that the actual value to patch in the command can be obtained.
