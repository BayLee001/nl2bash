%!TEX root=writeup.tex
\subsection{Natural Language to Bash Command Translation}
\label{subsec:parser}

We adopted a top-down approach that searches for the most relevant bash commands for a given natural language description. Specifically, we designed an efficient greedy search procedure (\S~\ref{subsec:decoding}) that enumerates the set of relevant bash commands generated by the bash grammar presented in \autoref{fig:lang}, and extracts a set of features for each of them paired with the natural language description (\S~\ref{subsec:feature}). The set of candidate commands are then ranked by a linear scoring function, the weights of which are learned from a training set (\S~\ref{subsec:training}). 

\subsubsection{Decoding}
\label{subsec:decoding}

To control scalability, we consider only the following small set of head commands: \texttt{find, mv, sort, grep, cp, ls, tar, xargs}. Nevertheless, the number of possible bash commands covered by this set is still daunting. Take the \texttt{find} command as an example, it has nearly 40 options and hence $2^{40}$ bash command candidates\footnote{Most of the command-option combinations are pretty rare. However, we haven't found an effective way to guide search with this information}. To prevent the search space from blowing up, we consider only candidate commands that contain $2\sim 5$ terms (a ``term'' is either a head command or an option). Furthermore, we use a greedy search procedure which only explores the top-$k$ best children of each node based on the scoring of the partial command ending at the child node. We designed the features to be factorized over each term of the program, i.e. a set of features are extracted for each term in the command candidate and the final feature set is the union of them. This allows features of a bigger program structure to be computed based on the features of its substructures. While this simplified feature set misses most information regarding both the structure of the commands and the structure of the language, we found it to frequently capture the gist of a translation in practice. 

The greedy procedure for bash command enumeration and feature extraction is summarized in alg.~\ref{alg:decoding}.

\begin{algorithm} [ht]
\caption{Bash command enumeration and feature extraction\label{alg:decoding}}
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\SetKwInOut{Initial}{Initialization}
\SetKwFunction{Add}{AddTo}
\SetKwFunction{DFS}{DFS}
\SetKwFunction{ExtractFeat}{ExtractFeatures}
\SetKwFunction{TopKBest}{TopKBestNextTerms}
\SetKwFunction{EndOfCmd}{IsEndOfCmd}
\SetKwFunction{PrintCmd}{GetCmdAndFeaturesWithBacktracking}

\Input{nl\_cmd, HeadCmdSet}
\Output{CmdsAndFeatures}
\Initial{CmdsAndFeatures=\{\}}
\For {head\_cmd $\in$ HeadCmdSet}{
	\DFS{head\_cmd, nl\_cmd}
}
\SetKwProg{fun}{Procedure}{}{}
\fun{\DFS{term, nl\_cmd}}{	
	term.features = \ExtractFeat{term, nl\_cmd} \\
	\If {\EndOfCmd{term}} {
		CmdsAndFeatures $\leftarrow$ CmdsAndFeatures + \\
			\PrintCmd{term}
	}
	\For {next\_term $\in$ \TopKBest{term}} {
		next\_term.backpointer = term \\
		\DFS{next\_term, nl\_cmd}
	}
}
\fun{\ExtractFeat{term, nl\_cmd}}{
	FeatureSet = $\emptyset$
	
	\For {word $\in$ nl\_cmd} {
		FeatureSet = FeatureSet + (term, word)	
	}
	\KwRet{FeatureSet}
}
\end{algorithm}

\subsubsection{Features}
\label{subsec:feature}
% The following features are used:
% \begin{itemize}\itemsep-1pt
%	\item association of key words/phrases to partial expressions
%  	\item association between partial expressions (e.g. how often do they combined in valid commands)
%	\item similarity of key words/phrases in the command to the man page explanation text of a partial expression
%	\item complexity of the logical formulas and the commands generated from them.
% \end{itemize}
As shown in the bottom function in alg.~\ref{alg:decoding}, the current feature set only consists of simple term-word tuples. We replaced any term unseen in the training set with the ``unk\_term'' symbol and any word unseen with the ``unk\_word'' symbol. To discriminate important features from unimportant ones, we assigned each tuple its PMI value, which is computed on the training set using the following equations:

\begin{align}
	PMI(t, w) &= \log{\frac{P^2(t,w)}{P(t)P(w)}} \\
	P(t, w) &= \frac{count\_of\_(t,w)}{total\_count\_of\_tuples} \\
	P(t) &= \frac{count\_of\_t}{total\_count\_of\_terms} \\
	P(w) &= \frac{count\_of\_w}{total\_count\_of\_words}.
\end{align}

The PMI value measures the association between a pair of term and word based on the training set. It has the effect of down-weighting uninformative tuples such as ``(\texttt{find}, and)''. However, we also noticed some anomalies associated with this approach. For example, the tuple ``(\texttt{-f}, file)'' has a very low PMI value, although the \texttt{-f} option literally refers to a file. This is because the word ``file'' occurs in almost every training example in our dataset (with and without \texttt{-f} present). Therefore simple example-level occurrence measure wasn't able to reveal the connection between \texttt{-f} and ``file''. What we actually need is statistical alignment, which infers the probability of the alignment between each term and word in every example, instead of doing exhaustive pair-up. The design of this alignment algorithm is left to future work. 

\subsubsection{Training}
\label{subsec:training}

We use the structured perception algorithm (alg.~\ref{alg:training}) to learn weights of the scoring function from the example pairs. In the training process, the input are pairs collected from StackOverflow, and the target is to learn a scoring functions to evaluate the correspondence between the a natural language description and a bash command. In each training iteration, top ranked logical form are selected and the weights are updated based on its similarity to the ground truth logical form. 

\begin{algorithm} 
\caption{Perceptron Training\label{alg:training}}
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\SetKwFunction{Feat}{FeatureVectorOf}
\SetKwFunction{TopPred}{TopPrediction}
\Input{$\{(bash\_cmd_i,nl\_cmd_i)|i=1\hdots N\}$}
\Output{weights}
\For{$t = 1,\hdots, T$}{
	\For{$i=1\hdots N$} {
		prediction = \TopPred{$nl\_cmd_i$} 
	
		\If {prediction != $bash\_cmd_i$} {
			weights = weights + 2 * \Feat{$bash\_cmd_i$} - \Feat{prediction}		
		}
	}
}
\end{algorithm}