\subsection{Natural Language to Bash Command Translation}
\label{subsec:parser}

We adopted a top-down approach that searches for the most relevant bash commands for a given natural language description. Specifically, we designed an efficient greedy search procedure (\S~\ref{subsec:decoding}) that enumerates the set of relevant bash commands generated by the DSL grammar developed in the previous section, and extracts a set of features for each of them paired with the natural language description (\S~\ref{subsec:feature}). The set of candidate programs are then ranked by a linear scoring function, the weights of which are learned from a training set (\S~\ref{subsec:training}). 

\subsubsection{Decoding}
\label{subsec:decoding}

To control scalability, we consider only the following small set of head commands: \texttt{find, mv, sort, grep, cp, ls, tar, xargs}. Nevertheless, the number of possible bash commands covered by this set is still daunting. Take the \texttt{find} command as an example, it has nearly 40 options and hence $2^{40}$ bash command candidates\footnote{Most of the command-option combinations are pretty rare. However, we haven't found an effective way to guide search with this information}. To prevent the search space from blowing up, we consider only candidate programs that contain $2\sim 5$ terms (a ``term'' is either a head command or an option). We also employ a greedy search procedure which only explores the top-$k$ best children of each node based on the scoring of the partial command ending at the child node. We designed the features to be factorized over each term of the program, i.e. a set of features are extracted for each term in the command candidate and the final feature set is the union of them. This allows us features of bigger structures to be computed based on the features of its substructures. While this simplified feature set misses most information regarding both the structure of the program and the structure of the language, we found it to frequently capture the gist of a translation in practice. 

The greedy procedure for bash command enumeration and feature extraction is summarized in alg.~\ref{alg:decoding}.

\begin{algorithm} [ht]
\caption{Bash command enumeration and feature extraction\label{alg:decoding}}
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\SetKwInOut{Initial}{Initialization}
\SetKwFunction{Add}{AddTo}
\SetKwFunction{DFS}{DFS}
\SetKwFunction{ExtractFeat}{ExtractFeatures}
\SetKwFunction{TopKBest}{TopKBestNextTerms}
\SetKwFunction{EndOfCmd}{IsEndOfCmd}
% \SetKwFunction{AccFeat}{AccFeaturesWithBacktracking}
\SetKwFunction{PrintCmd}{GetCmdAndFeaturesWithBacktracking}

\Input{nl\_cmd, HeadCmdSet}
\Output{CmdsAndFeatures}
\Initial{CmdsAndFeatures=\{\}}
\For {head\_cmd $\in$ HeadCmdSet}{
	\DFS{head\_cmd, nl\_cmd}
}
\SetKwProg{fun}{Procedure}{}{}
\fun{\DFS{term, nl\_cmd}}{	
	\ExtractFeat{term, nl\_cmd} \\
	\If {\EndOfCmd{term}} {
		CmdsAndFeatures $\leftarrow$ CmdsAndFeatures + \\
			\PrintCmd{term}
	}
	\For {next\_term $\in$ \TopKBest{term}} {
		next\_term.backpointer = term \\
		\DFS{next\_term, nl\_cmd}
	}
}
\fun{\ExtractFeat{term, nl\_cmd}}{
	FeatureSet = $\emptyset$
	
	\For {word $\in$ nl\_cmd} {
		FeatureSet = FeatureSet + (term, word)	
	}
	\KwRet{FeatureSet}
}
\end{algorithm}

\subsubsection{Features}
\label{subsec:feature}
The following features are used:
\begin{itemize}\itemsep-1pt
	\item association of key words/phrases to partial expressions
	\item association between partial expressions (e.g. how often do they combined in valid commands)
	\item similarity of key words/phrases in the command to the man page explanation text of a partial expression
	\item complexity of the logical formulas and the commands generated from them.
\end{itemize}
We use the structured perception algorithm to learn weights of the scoring function from the example pairs. In the training process, the input are pairs collected from StackOverflow, and the target is to learn a scoring functions to evaluate the correspondence between the a natural language sentence and a CLI program. In each training iteration, top ranked logical form are selected and the weights are updated based on its similarity to the ground truth logical form. 

\subsubsection{Training}
\label{subsec:training}

\begin{algorithm} 
\label{alg:training}
\caption{Perceptron Training}
\end{algorithm}